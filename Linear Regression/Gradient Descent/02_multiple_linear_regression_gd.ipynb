{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.570563</td>\n",
       "      <td>1.420342</td>\n",
       "      <td>0.495580</td>\n",
       "      <td>-9.763182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.990563</td>\n",
       "      <td>0.556965</td>\n",
       "      <td>1.045064</td>\n",
       "      <td>-24.029355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.674728</td>\n",
       "      <td>0.150617</td>\n",
       "      <td>1.774645</td>\n",
       "      <td>45.616421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.388250</td>\n",
       "      <td>-0.387127</td>\n",
       "      <td>-0.110229</td>\n",
       "      <td>34.135737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.167882</td>\n",
       "      <td>-0.024104</td>\n",
       "      <td>0.145063</td>\n",
       "      <td>86.663647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3     target\n",
       "0 -0.570563  1.420342  0.495580  -9.763182\n",
       "1 -0.990563  0.556965  1.045064 -24.029355\n",
       "2 -0.674728  0.150617  1.774645  45.616421\n",
       "3  0.388250 -0.387127 -0.110229  34.135737\n",
       "4  1.167882 -0.024104  0.145063  86.663647"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/campusx-official/linear-regression-assumptions/refs/heads/main/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"target\",axis=1).values\n",
    "y = df[\"target\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultipleLinearRegressionGD:\n",
    "\n",
    "    def __init__(self, epochs=1000, learning_rate=0.01):\n",
    "        self.intercept = 0  # Bias term\n",
    "        self.coeffs = None  # Coefficients for features\n",
    "        self.epochs = epochs\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize coefficients to zeros (or small random values)\n",
    "        self.coeffs = np.ones(X.shape[1])\n",
    "        n = len(X)  # Number of samples\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # Predictions\n",
    "            y_pred = self.intercept + np.dot(X, self.coeffs)\n",
    "            \n",
    "            # Errors\n",
    "            error = y - y_pred\n",
    "\n",
    "            # Gradients\n",
    "            dm = (-2/n) * np.dot(X.T, error)  # Gradient for coefficients\n",
    "            db = (-2/n) * np.sum(error)       # Gradient for intercept\n",
    "\n",
    "            # Update parameters\n",
    "            self.intercept -= self.lr * db\n",
    "            self.coeffs -= self.lr * dm\n",
    "\n",
    "        print(\"Weights are:\")\n",
    "        print(self.coeffs)\n",
    "\n",
    "        print(\"-----------\")\n",
    "\n",
    "        print(\"Bias is:\")\n",
    "        print(self.intercept)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Include the intercept in predictions\n",
    "        return self.intercept + np.dot(X, self.coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are:\n",
      "[ 7.39943877e+01 -4.15645712e-02  5.38326058e+01]\n",
      "-----------\n",
      "Bias is:\n",
      "0.2502409375138604\n"
     ]
    }
   ],
   "source": [
    "mlr = MultipleLinearRegressionGD(epochs=100,learning_rate=0.1)\n",
    "mlr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9605512034455976"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = mlr.predict(X)\n",
    "r2_score(y,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "N Rows:\n",
    "\n",
    "Updates\n",
    "\n",
    "=> 1 Epoch : N Updates\n",
    "\n",
    "=> 2 Epoch : N Updates\n",
    "\n",
    "and so on\n",
    "\n",
    "We dont have to load all data at once on machine as we did in batch GD, so, memory required will be low. And faster because it requires less number of epochs. Since we are updating whole data in each epoch. It means the time complexity is O(epoch*rows).\n",
    "\n",
    "Stochastic means having a random probability distribution or pattern that may be analysed statistically but may not be predicted precisely. So we pick randomly rows in each epochs. So we will have noises while convergence. So we use this when we have big data. But if the dataset is small we should use Batch GD as it is not noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_regression(n_samples=10000,n_features=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultipleLinearRegressionSGD:\n",
    "\n",
    "    def __init__(self, epochs=1000, learning_rate=0.01):\n",
    "        self.intercept = 0  # Bias term\n",
    "        self.coeffs = None  # Coefficients for features\n",
    "        self.epochs = epochs\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize coefficients to zeros (or small random values)\n",
    "        self.coeffs = np.ones(X.shape[1])\n",
    "        n = len(X)  # Number of samples\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(n):\n",
    "                random_idx = np.random.randint(0,n)\n",
    "                # Prediction\n",
    "                y_pred = self.intercept + np.dot(X[random_idx], self.coeffs)\n",
    "                \n",
    "                # Errors\n",
    "                error = y[random_idx] - y_pred\n",
    "\n",
    "                # Gradients\n",
    "                dm = np.dot(X[random_idx].T, error)  # Gradient for coefficients\n",
    "                db = error      # Gradient for intercept\n",
    "\n",
    "                # Update parameters\n",
    "                self.intercept -= self.lr * db\n",
    "                self.coeffs -= self.lr * dm\n",
    "\n",
    "        print(\"Weights are:\")\n",
    "        print(self.coeffs)\n",
    "\n",
    "        print(\"-----------\")\n",
    "\n",
    "        print(\"Bias is:\")\n",
    "        print(self.intercept)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Include the intercept in predictions\n",
    "        return self.intercept + np.dot(X, self.coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are:\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan]\n",
      "-----------\n",
      "Bias is:\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "mlr = MultipleLinearRegressionSGD()\n",
    "mlr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Saurav\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:1275: RuntimeWarning: overflow encountered in square\n",
      "  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = mlr.predict(X)\n",
    "r2_score(y,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue we're encountering, where the weights and bias become NaN, is likely due to exploding gradients caused by a combination of:\n",
    "\n",
    "High Learning Rate:\n",
    "\n",
    "If the learning rate is too large, the updates to the weights and bias can become extremely large, leading to numerical instability and NaN values.\n",
    "\n",
    "Unscaled Data:\n",
    "\n",
    "The make_regression dataset may have features with very different scales. If the features are not normalized, gradients can become very large, causing updates to diverge.\n",
    "\n",
    "Large Dataset Size:\n",
    "\n",
    "With n_samples=10000 and n_features=20, the gradients can accumulate to very large values if the learning rate is not appropriately scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultipleLinearRegressionSGD:\n",
    "    def __init__(self, epochs=1000, learning_rate=0.001):\n",
    "        self.intercept = 0  # Bias term\n",
    "        self.coeffs = None  # Coefficients for features\n",
    "        self.epochs = epochs\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.coeffs = np.ones(X.shape[1])\n",
    "        n = len(X)  # Number of samples\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for j in range(n):\n",
    "                random_idx = np.random.randint(0, n)\n",
    "                # Prediction\n",
    "                y_pred = self.intercept + np.dot(X[random_idx], self.coeffs)\n",
    "                \n",
    "                # Errors\n",
    "                error = y[random_idx] - y_pred\n",
    "\n",
    "                # Gradients\n",
    "                dm = -2*np.dot(X[random_idx].T, error)  # Gradient for coefficients\n",
    "                db = -2*error  # Gradient for intercept\n",
    "\n",
    "                # Update parameters\n",
    "                self.intercept -= self.lr * db\n",
    "                self.coeffs -= self.lr * dm\n",
    "\n",
    "        print(\"Weights are:\")\n",
    "        print(self.coeffs)\n",
    "\n",
    "        print(\"-----------\")\n",
    "\n",
    "        print(\"Bias is:\")\n",
    "        print(self.intercept)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.intercept + np.dot(X, self.coeffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are:\n",
      "[ 9.27834005e-16  8.33551620e-15  9.32293671e+01  9.27498527e+01\n",
      " -4.84832148e-16 -5.03139797e-15  8.35958806e+01  4.45411762e+01\n",
      "  8.34933559e+01 -7.47850383e-16  9.38078963e+01  1.31446443e-15\n",
      "  4.15273702e+01 -5.48335925e-15  4.22029328e-15  4.97857528e+01\n",
      " -6.61007125e-15  3.15802527e-16  9.62359244e+01  2.58791695e+01]\n",
      "-----------\n",
      "Bias is:\n",
      "9.76716249329481\n"
     ]
    }
   ],
   "source": [
    "X,y = make_regression(n_samples=1000,n_features=20)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Train model\n",
    "mlr = MultipleLinearRegressionSGD(learning_rate=0.01)\n",
    "mlr.fit(X_normalized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9976797624493334"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = mlr.predict(X)\n",
    "r2_score(y,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 47, 98,  2, 77, 83, 72,  4, 10, 14], dtype=int32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_idx = np.random.randint(0, 100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[random_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultipleLinearRegressionSGD:\n",
    "    def __init__(self, epochs=1000, learning_rate=0.01,batch_size=32):\n",
    "        self.intercept = 0  # Bias term\n",
    "        self.coeffs = None  # Coefficients for features\n",
    "        self.epochs = epochs\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.coeffs = np.ones(X.shape[1])\n",
    "        n = len(X)  # Number of samples\n",
    "\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for j in range(self.batch_size):\n",
    "                random_idx = np.random.randint(0, n,self.batch_size)\n",
    "                # Prediction\n",
    "                y_pred = self.intercept + np.dot(X[random_idx], self.coeffs)\n",
    "                \n",
    "                # Errors\n",
    "                error = y[random_idx] - y_pred\n",
    "\n",
    "                # Gradients and dividing by batch size\n",
    "                dm = (-2/self.batch_size) * np.dot(X[random_idx].T, error)  # Gradient for coefficients\n",
    "                db = (-2/self.batch_size) * np.sum(error)  # Gradient for intercept\n",
    "\n",
    "                # Update parameters\n",
    "                self.intercept -= self.lr * db\n",
    "                self.coeffs -= self.lr * dm\n",
    "\n",
    "        print(\"Weights are:\")\n",
    "        print(self.coeffs)\n",
    "\n",
    "        print(\"-----------\")\n",
    "\n",
    "        print(\"Bias is:\")\n",
    "        print(self.intercept)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.intercept + np.dot(X, self.coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are:\n",
      "[-2.36537569e-15  7.18634488e+01 -9.14247880e-15  8.30346930e-15\n",
      " -8.45705060e-15  4.64663413e+01  1.04968913e-14 -2.05800740e-18\n",
      "  3.68358306e+01 -2.15138824e-14  9.27134037e+01  2.85590424e+01\n",
      "  3.33374419e+00  8.59081724e+01  6.34505521e+01  2.30477572e-15\n",
      "  5.78385515e+01  1.08658237e-14 -7.84949272e-15  2.67546373e+01]\n",
      "-----------\n",
      "Bias is:\n",
      "9.009016433945385\n"
     ]
    }
   ],
   "source": [
    "X,y = make_regression(n_samples=1000,n_features=20)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Train model\n",
    "mlr = MultipleLinearRegressionSGD(learning_rate=0.01)\n",
    "mlr.fit(X_normalized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9971572633209864"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = mlr.predict(X)\n",
    "r2_score(y,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
